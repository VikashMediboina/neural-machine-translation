{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import random\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import spacy\n",
    "import torchtext\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "english_sentences_train = read_text_file('/home/mediboina.v/Vikash/Deeplearning/neural-machine-translation/data/train.en')\n",
    "vietnamese_sentences_train = read_text_file('/home/mediboina.v/Vikash/Deeplearning/neural-machine-translation/data/train.vi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences_val = read_text_file('/home/mediboina.v/Vikash/Deeplearning/neural-machine-translation/data/tst2013.en')\n",
    "vietnamese_sentences_val = read_text_file('/home/mediboina.v/Vikash/Deeplearning/neural-machine-translation/data/tst2013.vi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load(\"en_core_web_md\")\n",
    "def tokenize_vietnamese(text):\n",
    "    return ViTokenizer.tokenize(text).split()\n",
    "\n",
    "# Function to tokenize English text using spaCy\n",
    "def tokenize_english(input_texts):\n",
    "    return [[token.text for token in nlp_en(text)] for text in input_texts]\n",
    "\n",
    "def tokenize(vietnamese_sentences,english_sentences):\n",
    "\n",
    "\n",
    "     # Tokenize all sentences\n",
    "    tokenized_english = tokenize_english(english_sentences)\n",
    "    tokenized_vietnamese = [tokenize_vietnamese(sentence) for sentence in vietnamese_sentences]\n",
    "\n",
    "    return tokenized_english,tokenized_vietnamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_english, tokenized_vietnamese \u001b[39m=\u001b[39mtokenize(english_sentences_train,vietnamese_sentences_train)\n",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(vietnamese_sentences, english_sentences)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(vietnamese_sentences,english_sentences):\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m      \u001b[39m# Tokenize all sentences\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     tokenized_english \u001b[39m=\u001b[39m tokenize_english(english_sentences)\n\u001b[1;32m     14\u001b[0m     tokenized_vietnamese \u001b[39m=\u001b[39m [tokenize_vietnamese(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m vietnamese_sentences]\n\u001b[1;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenized_english,tokenized_vietnamese\n",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m, in \u001b[0;36mtokenize_english\u001b[0;34m(input_texts)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_english\u001b[39m(input_texts):\n\u001b[0;32m----> 7\u001b[0m     \u001b[39mreturn\u001b[39;00m [[token\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nlp_en(text)] \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m input_texts]\n",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_english\u001b[39m(input_texts):\n\u001b[0;32m----> 7\u001b[0m     \u001b[39mreturn\u001b[39;00m [[token\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nlp_en(text)] \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m input_texts]\n",
      "File \u001b[0;32m~/.conda/envs/environment/lib/python3.10/site-packages/spacy/language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[1;32m   1048\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1049\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg\u001b[39m.\u001b[39;49mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1051\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenized_english, tokenized_vietnamese =tokenize(english_sentences_train,vietnamese_sentences_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenized data to pickle files\n",
    "with open('data/train_en_toknized.pkl', 'wb') as f:\n",
    "        pickle.dump(tokenized_english, f)\n",
    "\n",
    "with open('data/train_vi_toknized.pkl', 'wb') as f:\n",
    "        pickle.dump(tokenized_vietnamese, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokenized_english, val_tokenized_vietnamese =tokenize(english_sentences_val,vietnamese_sentences_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenized data to pickle files\n",
    "with open('data/test_en_toknized.pkl', 'wb') as f:\n",
    "        pickle.dump(val_tokenized_english, f)\n",
    "\n",
    "with open('data/test_vi_toknized.pkl', 'wb') as f:\n",
    "        pickle.dump(val_tokenized_vietnamese, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24367, 55694)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open('data/train_en_toknized.pkl', 'rb') as f:\n",
    "        english_tokenized = pickle.load(f)\n",
    "\n",
    "    # Load German tokenized sentences from the pickle file\n",
    "with open('data/train_vi_toknized.pkl', 'rb') as f:\n",
    "        vi_tokenized = pickle.load(f)\n",
    "\n",
    "with open('data/test_en_toknized.pkl', 'rb') as f:\n",
    "        val_english_tokenized = pickle.load(f)\n",
    "\n",
    "    # Load vi tokenized sentences from the pickle file\n",
    "with open('data/test_vi_toknized.pkl', 'rb') as f:\n",
    "        val_vi_tokenized = pickle.load(f)\n",
    "\n",
    "\n",
    "all_english = english_tokenized + val_english_tokenized \n",
    "all_vi = vi_tokenized + val_vi_tokenized\n",
    "# Create vocabulary\n",
    "eng_vocab = torchtext.vocab.build_vocab_from_iterator(all_english)\n",
    "ger_vocab = torchtext.vocab.build_vocab_from_iterator(all_vi)\n",
    "# Save vocabularies\n",
    "with open('data/en_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(eng_vocab, f)\n",
    "\n",
    "with open('data/vi_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(ger_vocab, f)\n",
    "len(eng_vocab),len(ger_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_from_tokenized_data(tokenized_data):\n",
    "    return [[token for token in sentence if token not in [',', '.']] for sentence in tokenized_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/en_vocab.pkl', 'rb') as f:\n",
    "    eng_vocab = pickle.load(f)\n",
    "\n",
    "# Load viman tokenized sentences from the pickle file\n",
    "with open('data/vi_vocab.pkl', 'rb') as f:\n",
    "    vi_vocab = pickle.load(f)\n",
    "\n",
    "if '<unk>' not in eng_vocab:\n",
    "    eng_vocab.insert_token('<unk>', 0)  # Adjust the index if needed\n",
    "\n",
    "if '<unk>' not in vi_vocab:\n",
    "    vi_vocab.insert_token('<unk>', 0)  # Adjust the index if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_data_from_pickle(file_name):\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_name, 'rb') as f:\n",
    "            while True:\n",
    "                try:\n",
    "                    data.extend(pickle.load(f))\n",
    "                except EOFError:\n",
    "                    break\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_name}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess( english_tokenized_file_name,vi_tokenized_file_name,eng_vocab,ger_vocab):\n",
    "    english_tokenized=read_all_data_from_pickle(english_tokenized_file_name)\n",
    "    vi_tokenized=read_all_data_from_pickle(vi_tokenized_file_name)\n",
    "\n",
    "    english_tokenized = remove_punctuation_from_tokenized_data(english_tokenized)\n",
    "    vi_tokenized = remove_punctuation_from_tokenized_data(vi_tokenized)\n",
    "    # Convert words to indices\n",
    "    english_indices = [torch.tensor([eng_vocab[word] if word in eng_vocab else eng_vocab['<unk>'] for word in sentence], dtype=torch.long) for sentence in english_tokenized]\n",
    "    vi_indices = [torch.tensor([ger_vocab[word] if word in ger_vocab else eng_vocab['<unk>'] for word in sentence], dtype=torch.long) for sentence in vi_tokenized]\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    max_len = max(max(len(seq) for seq in english_indices), max(len(seq) for seq in vi_indices))\n",
    "    english_padded = pad_sequence([torch.cat([seq, torch.zeros(max_len - len(seq))], dim=0) for seq in english_indices], batch_first=True)\n",
    "    vi_padded = pad_sequence([torch.cat([seq, torch.zeros(max_len - len(seq))], dim=0) for seq in vi_indices], batch_first=True)\n",
    "\n",
    "    return english_padded,vi_padded, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en_tokenize_file_name='data/train_en_toknized.pkl'\n",
    "train_vi_tokenize_file_name='data/train_vi_toknized.pkl'\n",
    "val_en_tokenize_file_name='data/test_en_toknized.pkl'\n",
    "val_vi_tokenize_file_name='data/test_vi_toknized.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_english_padded,train_vi_padded,max_len = preProcess(train_en_tokenize_file_name,train_vi_tokenize_file_name,eng_vocab,ger_vocab)\n",
    "val_english_padded,val_vi_padded,max_len = preProcess(val_en_tokenize_file_name,val_vi_tokenize_file_name,eng_vocab,ger_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133317, 133317, 1268, 1268)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_english_padded),len(train_vi_padded),len(val_english_padded),len(val_vi_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_english_padded = train_english_padded.to(device)\n",
    "train_vi_padded = train_vi_padded.to(device)\n",
    "val_english_padded = val_english_padded.to(device)\n",
    "val_vi_padded = val_vi_padded.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, english_data, vi_data):\n",
    "        self.english_data = english_data\n",
    "        self.vi_data = vi_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.english_data[idx], self.vi_data[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133317 133317\n"
     ]
    }
   ],
   "source": [
    "print(len(train_english_padded),len(train_vi_padded))\n",
    "# Create DataLoader\n",
    "train_dataset = TranslationDataset(train_english_padded, train_vi_padded)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create DataLoader\n",
    "val_dataset = TranslationDataset(val_english_padded, val_vi_padded)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x.long()))\n",
    "        # embedding shape: (seq_length, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        return hidden, cell\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size=output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape: (N) but we want (1, N)\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x.long()))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # outputs shape: (1, N, hidden_size)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "        # predictions shape: (1, N, length_of_vocab)\n",
    "\n",
    "        predictions = predictions.squeeze(0)\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio = 0.5):\n",
    "        # source shape: (src_len, N)\n",
    "        # target shape: (trg_len, N)\n",
    "\n",
    "        trg_len, N = target.shape\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, N, trg_vocab_size).to(device)\n",
    "\n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        x = target[0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "\n",
    "            outputs[t] = output\n",
    "\n",
    "            # decide if we will use teacher forcing or not\n",
    "            best_guess = output.argmax(1)\n",
    "            x = target[t] if random.random() < teacher_forcing_ratio else best_guess\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24368 55695\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "INPUT_DIM = len(eng_vocab)  # Assuming eng_vocab is your English vocabulary\n",
    "OUTPUT_DIM = len(vi_vocab)  # Assuming ger_vocab is your German vocabulary\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 1024\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "learning_rate=0.001\n",
    "print(INPUT_DIM,OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enc = EncoderLSTM(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
    "dec = DecoderLSTM(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, OUTPUT_DIM, N_LAYERS, DEC_DROPOUT).to(device)\n",
    "start_epoch=0\n",
    "model = Seq2Seq(enc, dec).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0) \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('data/model_checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "start_epoch = checkpoint['epoch'] + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        \n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "       \n",
    "        # trg = [trg len, batch size]\n",
    "        # output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "       \n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1).long()\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "def evaluate(model, iterator, criterion,ger_vocab):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    predictions, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for _, (src, trg) in enumerate(iterator):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "            output = model(src, trg, 0)  # turn off teacher forcing\n",
    "            \n",
    "\n",
    "            # Convert output to token indices\n",
    "            output_indices = output.argmax(2)  # Choose the word with highest probability\n",
    "            output_sentences = tensor_to_sentence(output_indices[:, 1:], ger_vocab)\n",
    "            predictions.extend(output_sentences)\n",
    "\n",
    "            # Convert trg to token strings, skipping <sos> token\n",
    "            trg_sentences = tensor_to_sentence(trg[:, 1:], ger_vocab)\n",
    "            targets.extend([[sent] for sent in trg_sentences])  # Wrap each sentence in another list\n",
    "\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1).long()\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    bleu=corpus_bleu( targets,predictions)\n",
    "    return epoch_loss / len(iterator) , bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_sentence(tensor, vocab, pad_index=0, eos_index=None, sos_index=None):\n",
    "    itos = vocab.get_itos()\n",
    "    sentences = []\n",
    "    \n",
    "    for i in range(tensor.size(0)):  # Loop over each item in the batch\n",
    "        sentence = []\n",
    "        for idx in tensor[i]:\n",
    "            if idx == pad_index or idx == eos_index or idx == sos_index:\n",
    "                continue  # Skip pad, eos, and sos tokens\n",
    "            sentence.append(itos[int(idx.item())])\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.32 GiB. GPU 0 has a total capacty of 31.74 GiB of which 273.62 MiB is free. Process 111737 has 16.31 GiB memory in use. Including non-PyTorch memory, this process has 15.16 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 68.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m best_val_loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_epoch,N_EPOCHS):\n\u001b[0;32m----> 6\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_dataloader, optimizer, criterion, CLIP)\n\u001b[1;32m      7\u001b[0m     valid_loss, val_bleu \u001b[39m=\u001b[39m evaluate(model, val_dataloader, criterion,vi_vocab)\n\u001b[1;32m      8\u001b[0m     \u001b[39mif\u001b[39;00m valid_loss \u001b[39m<\u001b[39m best_val_loss:\n",
      "Cell \u001b[0;32mIn[59], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m      8\u001b[0m src, trg \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mto(device), trg\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m output \u001b[39m=\u001b[39m model(src, trg)\n\u001b[1;32m     14\u001b[0m \u001b[39m# trg = [trg len, batch size]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# output = [trg len, batch size, output dim]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m output_dim \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/environment/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/environment/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[56], line 64\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, source, target, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     61\u001b[0m outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(trg_len, N, trg_vocab_size)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     63\u001b[0m \u001b[39m# last hidden state of the encoder is used as the initial hidden state of the decoder\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m hidden, cell \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(source)\n\u001b[1;32m     66\u001b[0m \u001b[39m# first input to the decoder is the <sos> tokens\u001b[39;00m\n\u001b[1;32m     67\u001b[0m x \u001b[39m=\u001b[39m target[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/environment/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/environment/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[56], line 16\u001b[0m, in \u001b[0;36mEncoderLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x\u001b[39m.\u001b[39mlong()))\n\u001b[1;32m     14\u001b[0m \u001b[39m# embedding shape: (seq_length, N, embedding_size)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m outputs, (hidden, cell) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(embedding)\n\u001b[1;32m     17\u001b[0m \u001b[39mreturn\u001b[39;00m hidden, cell\n",
      "File \u001b[0;32m~/.conda/envs/environment/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/environment/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/environment/lib/python3.10/site-packages/torch/nn/modules/rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m         hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    880\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    881\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.32 GiB. GPU 0 has a total capacty of 31.74 GiB of which 273.62 MiB is free. Process 111737 has 16.31 GiB memory in use. Including non-PyTorch memory, this process has 15.16 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 68.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "N_EPOCHS = 50\n",
    "CLIP = 1\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(start_epoch,N_EPOCHS):\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "    valid_loss, val_bleu = evaluate(model, val_dataloader, criterion,vi_vocab)\n",
    "    if valid_loss < best_val_loss:\n",
    "        best_val_loss = valid_loss\n",
    "        # Save the model\n",
    "        torch.save(model.state_dict(), 'data/eng_to_vi_translation_model.pth')\n",
    "        print(\"Saved Best Model\")\n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Val. Loss: {valid_loss:.3f}, Valid Bleu: {val_bleu:.5f}')\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'data/eng_to_vi_translation_model_end.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'data/eng_to_vi_translation_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 6.297, Val. Loss: 6.798, Valid Bleu: 0.00015\n"
     ]
    }
   ],
   "source": [
    " print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Val. Loss: {valid_loss:.3f}, Valid Bleu: {val_bleu:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    'epoch': epoch,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    # Include any other information you want\n",
    "}\n",
    "torch.save(state, 'data/model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
