{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext.legacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtranslate\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbleu_score\u001b[39;00m \u001b[39mimport\u001b[39;00m corpus_bleu\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m Counter\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Field\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext.legacy'"
     ]
    }
   ],
   "source": [
    "import spacy_transformers\n",
    "import spacy\n",
    "from spacy.lang.en.examples import sentences \n",
    "import torchtext\n",
    "import pickle\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from collections import Counter\n",
    "from torchtext.legacy.data import Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m nlp_de \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mde_core_news_md\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtrain.json\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> 6\u001b[0m    train_data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mload(f)\n",
      "File \u001b[0;32m~/.conda/envs/environment/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(fp, \u001b[39m*\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_float\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_pairs_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39;49mread(),\n\u001b[1;32m    294\u001b[0m         \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m, object_hook\u001b[39m=\u001b[39mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[39m=\u001b[39mparse_float, parse_int\u001b[39m=\u001b[39mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[39m=\u001b[39mparse_constant, object_pairs_hook\u001b[39m=\u001b[39mobject_pairs_hook, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "nlp_en = spacy.load(\"en_core_web_md\")\n",
    "nlp_de = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "with open('train.json') as f:\n",
    "   train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die ganze Stadt ist ein Startup: Shenzhen ist das Silicon Valley für Hardware-Firmen\n",
      "Die DET nk\n",
      "ganze ADJ nk\n",
      "Stadt NOUN sb\n",
      "ist AUX ROOT\n",
      "ein DET nk\n",
      "Startup NOUN pd\n",
      ": PUNCT punct\n",
      "Shenzhen PROPN sb\n",
      "ist AUX cj\n",
      "das DET nk\n",
      "Silicon PROPN pnc\n",
      "Valley PROPN sb\n",
      "für ADP mnr\n",
      "Hardware-Firmen NOUN nk\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('validation.json') as f:\n",
    "   val_data = json.load(f)\n",
    "with open('test.json') as f:\n",
    "   test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first for train data only\n",
    "data=train_data['data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en_tokenize_file_name='english_tokenized.pkl'\n",
    "train_de_tokenize_file_name='german_tokenized.pkl'\n",
    "val_en_tokenize_file_name='val_english_tokenized.pkl'\n",
    "val_de_tokenize_file_name='val_german_tokenized.pkl'\n",
    "test_en_tokenize_file_name='test_english_tokenized.pkl'\n",
    "test_de_tokenize_file_name='test_german_tokenized.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4522998"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_tokenize_input(input_texts, nlp_eng):\n",
    "    # Tokenize the input texts in batches\n",
    "    input_tokenized = [nlp_eng(text) for text in input_texts]\n",
    "    return [[token.text for token in doc] for doc in input_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first Time\n",
    "def tokenize(data,en_file_name,de_file_name):\n",
    "    german_sentences = [item['de'] for item in data]\n",
    "    english_sentences = [item['en'] for item in data]\n",
    "    # Batch size for tokenization\n",
    "    batch_size = 1000\n",
    "\n",
    "\n",
    "    # Tokenize and save in batches\n",
    "    for i in range(0, len(english_sentences), batch_size):\n",
    "        batch_english = batch_tokenize_input(english_sentences[i:i+batch_size], nlp_en)\n",
    "        batch_german = batch_tokenize_input(german_sentences[i:i+batch_size], nlp_de)\n",
    "        print(batch_size,i)\n",
    "        # Append tokenized batches to the pickle file\n",
    "        with open(en_file_name, 'ab') as f:\n",
    "            pickle.dump(batch_english, f)\n",
    "\n",
    "        with open(de_file_name, 'ab') as f:\n",
    "            pickle.dump(batch_german, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0\n",
      "1000 1000\n",
      "1000 2000\n",
      "1000 0\n",
      "1000 1000\n",
      "1000 2000\n",
      "1000 3000\n",
      "1000 0\n",
      "1000 1000\n",
      "1000 2000\n",
      "1000 3000\n",
      "1000 4000\n",
      "1000 5000\n",
      "1000 6000\n",
      "1000 7000\n",
      "1000 8000\n",
      "1000 9000\n",
      "1000 10000\n",
      "1000 11000\n",
      "1000 12000\n",
      "1000 13000\n",
      "1000 14000\n",
      "1000 15000\n",
      "1000 16000\n",
      "1000 17000\n",
      "1000 18000\n",
      "1000 19000\n",
      "1000 20000\n",
      "1000 21000\n",
      "1000 22000\n",
      "1000 23000\n",
      "1000 24000\n",
      "1000 25000\n",
      "1000 26000\n",
      "1000 27000\n",
      "1000 28000\n",
      "1000 29000\n",
      "1000 30000\n",
      "1000 31000\n",
      "1000 32000\n",
      "1000 33000\n",
      "1000 34000\n",
      "1000 35000\n",
      "1000 36000\n",
      "1000 37000\n",
      "1000 38000\n",
      "1000 39000\n",
      "1000 40000\n",
      "1000 41000\n",
      "1000 42000\n",
      "1000 43000\n",
      "1000 44000\n",
      "1000 45000\n",
      "1000 46000\n",
      "1000 47000\n",
      "1000 48000\n",
      "1000 49000\n",
      "1000 50000\n",
      "1000 51000\n",
      "1000 52000\n",
      "1000 53000\n",
      "1000 54000\n",
      "1000 55000\n",
      "1000 56000\n",
      "1000 57000\n",
      "1000 58000\n",
      "1000 59000\n",
      "1000 60000\n",
      "1000 61000\n",
      "1000 62000\n",
      "1000 63000\n",
      "1000 64000\n",
      "1000 65000\n",
      "1000 66000\n",
      "1000 67000\n",
      "1000 68000\n",
      "1000 69000\n",
      "1000 70000\n",
      "1000 71000\n",
      "1000 72000\n",
      "1000 73000\n",
      "1000 74000\n",
      "1000 75000\n",
      "1000 76000\n",
      "1000 77000\n",
      "1000 78000\n",
      "1000 79000\n",
      "1000 80000\n",
      "1000 81000\n",
      "1000 82000\n",
      "1000 83000\n",
      "1000 84000\n",
      "1000 85000\n",
      "1000 86000\n",
      "1000 87000\n",
      "1000 88000\n",
      "1000 89000\n",
      "1000 90000\n",
      "1000 91000\n",
      "1000 92000\n",
      "1000 93000\n",
      "1000 94000\n",
      "1000 95000\n",
      "1000 96000\n",
      "1000 97000\n",
      "1000 98000\n",
      "1000 99000\n",
      "1000 100000\n",
      "1000 101000\n",
      "1000 102000\n",
      "1000 103000\n",
      "1000 104000\n",
      "1000 105000\n",
      "1000 106000\n",
      "1000 107000\n",
      "1000 108000\n",
      "1000 109000\n",
      "1000 110000\n",
      "1000 111000\n",
      "1000 112000\n",
      "1000 113000\n",
      "1000 114000\n",
      "1000 115000\n",
      "1000 116000\n",
      "1000 117000\n",
      "1000 118000\n",
      "1000 119000\n",
      "1000 120000\n",
      "1000 121000\n",
      "1000 122000\n",
      "1000 123000\n",
      "1000 124000\n",
      "1000 125000\n",
      "1000 126000\n",
      "1000 127000\n",
      "1000 128000\n",
      "1000 129000\n",
      "1000 130000\n",
      "1000 131000\n",
      "1000 132000\n",
      "1000 133000\n",
      "1000 134000\n",
      "1000 135000\n",
      "1000 136000\n",
      "1000 137000\n",
      "1000 138000\n",
      "1000 139000\n",
      "1000 140000\n",
      "1000 141000\n",
      "1000 142000\n",
      "1000 143000\n",
      "1000 144000\n",
      "1000 145000\n",
      "1000 146000\n",
      "1000 147000\n",
      "1000 148000\n",
      "1000 149000\n",
      "1000 150000\n",
      "1000 151000\n",
      "1000 152000\n",
      "1000 153000\n",
      "1000 154000\n",
      "1000 155000\n",
      "1000 156000\n",
      "1000 157000\n",
      "1000 158000\n",
      "1000 159000\n",
      "1000 160000\n",
      "1000 161000\n",
      "1000 162000\n",
      "1000 163000\n",
      "1000 164000\n",
      "1000 165000\n",
      "1000 166000\n",
      "1000 167000\n",
      "1000 168000\n",
      "1000 169000\n",
      "1000 170000\n",
      "1000 171000\n",
      "1000 172000\n",
      "1000 173000\n",
      "1000 174000\n",
      "1000 175000\n",
      "1000 176000\n",
      "1000 177000\n",
      "1000 178000\n",
      "1000 179000\n",
      "1000 180000\n",
      "1000 181000\n",
      "1000 182000\n",
      "1000 183000\n",
      "1000 184000\n",
      "1000 185000\n",
      "1000 186000\n",
      "1000 187000\n",
      "1000 188000\n",
      "1000 189000\n",
      "1000 190000\n",
      "1000 191000\n",
      "1000 192000\n",
      "1000 193000\n",
      "1000 194000\n",
      "1000 195000\n",
      "1000 196000\n",
      "1000 197000\n",
      "1000 198000\n",
      "1000 199000\n",
      "1000 200000\n",
      "1000 201000\n",
      "1000 202000\n",
      "1000 203000\n",
      "1000 204000\n",
      "1000 205000\n",
      "1000 206000\n",
      "1000 207000\n",
      "1000 208000\n",
      "1000 209000\n",
      "1000 210000\n",
      "1000 211000\n",
      "1000 212000\n",
      "1000 213000\n",
      "1000 214000\n",
      "1000 215000\n",
      "1000 216000\n",
      "1000 217000\n",
      "1000 218000\n",
      "1000 219000\n",
      "1000 220000\n",
      "1000 221000\n",
      "1000 222000\n",
      "1000 223000\n",
      "1000 224000\n",
      "1000 225000\n",
      "1000 226000\n",
      "1000 227000\n",
      "1000 228000\n",
      "1000 229000\n",
      "1000 230000\n",
      "1000 231000\n",
      "1000 232000\n",
      "1000 233000\n",
      "1000 234000\n",
      "1000 235000\n",
      "1000 236000\n",
      "1000 237000\n",
      "1000 238000\n",
      "1000 239000\n",
      "1000 240000\n",
      "1000 241000\n",
      "1000 242000\n",
      "1000 243000\n",
      "1000 244000\n",
      "1000 245000\n",
      "1000 246000\n",
      "1000 247000\n",
      "1000 248000\n",
      "1000 249000\n",
      "1000 250000\n",
      "1000 251000\n",
      "1000 252000\n",
      "1000 253000\n",
      "1000 254000\n",
      "1000 255000\n",
      "1000 256000\n",
      "1000 257000\n",
      "1000 258000\n",
      "1000 259000\n",
      "1000 260000\n",
      "1000 261000\n",
      "1000 262000\n",
      "1000 263000\n",
      "1000 264000\n",
      "1000 265000\n",
      "1000 266000\n",
      "1000 267000\n",
      "1000 268000\n",
      "1000 269000\n",
      "1000 270000\n",
      "1000 271000\n",
      "1000 272000\n",
      "1000 273000\n",
      "1000 274000\n",
      "1000 275000\n",
      "1000 276000\n",
      "1000 277000\n",
      "1000 278000\n",
      "1000 279000\n",
      "1000 280000\n",
      "1000 281000\n",
      "1000 282000\n",
      "1000 283000\n",
      "1000 284000\n",
      "1000 285000\n",
      "1000 286000\n",
      "1000 287000\n",
      "1000 288000\n",
      "1000 289000\n",
      "1000 290000\n",
      "1000 291000\n",
      "1000 292000\n",
      "1000 293000\n",
      "1000 294000\n",
      "1000 295000\n",
      "1000 296000\n",
      "1000 297000\n",
      "1000 298000\n",
      "1000 299000\n",
      "1000 300000\n",
      "1000 301000\n",
      "1000 302000\n",
      "1000 303000\n",
      "1000 304000\n",
      "1000 305000\n",
      "1000 306000\n",
      "1000 307000\n",
      "1000 308000\n",
      "1000 309000\n",
      "1000 310000\n",
      "1000 311000\n",
      "1000 312000\n",
      "1000 313000\n",
      "1000 314000\n",
      "1000 315000\n",
      "1000 316000\n",
      "1000 317000\n",
      "1000 318000\n",
      "1000 319000\n",
      "1000 320000\n",
      "1000 321000\n",
      "1000 322000\n",
      "1000 323000\n",
      "1000 324000\n",
      "1000 325000\n",
      "1000 326000\n",
      "1000 327000\n",
      "1000 328000\n",
      "1000 329000\n",
      "1000 330000\n",
      "1000 331000\n",
      "1000 332000\n",
      "1000 333000\n",
      "1000 334000\n",
      "1000 335000\n",
      "1000 336000\n",
      "1000 337000\n",
      "1000 338000\n",
      "1000 339000\n",
      "1000 340000\n",
      "1000 341000\n",
      "1000 342000\n",
      "1000 343000\n",
      "1000 344000\n",
      "1000 345000\n",
      "1000 346000\n",
      "1000 347000\n",
      "1000 348000\n",
      "1000 349000\n",
      "1000 350000\n",
      "1000 351000\n",
      "1000 352000\n",
      "1000 353000\n",
      "1000 354000\n",
      "1000 355000\n",
      "1000 356000\n",
      "1000 357000\n",
      "1000 358000\n",
      "1000 359000\n",
      "1000 360000\n",
      "1000 361000\n",
      "1000 362000\n",
      "1000 363000\n",
      "1000 364000\n",
      "1000 365000\n",
      "1000 366000\n",
      "1000 367000\n",
      "1000 368000\n",
      "1000 369000\n",
      "1000 370000\n",
      "1000 371000\n",
      "1000 372000\n",
      "1000 373000\n",
      "1000 374000\n",
      "1000 375000\n",
      "1000 376000\n",
      "1000 377000\n",
      "1000 378000\n",
      "1000 379000\n",
      "1000 380000\n",
      "1000 381000\n",
      "1000 382000\n",
      "1000 383000\n",
      "1000 384000\n",
      "1000 385000\n",
      "1000 386000\n",
      "1000 387000\n",
      "1000 388000\n",
      "1000 389000\n",
      "1000 390000\n",
      "1000 391000\n",
      "1000 392000\n",
      "1000 393000\n",
      "1000 394000\n",
      "1000 395000\n",
      "1000 396000\n",
      "1000 397000\n",
      "1000 398000\n",
      "1000 399000\n",
      "1000 400000\n",
      "1000 401000\n",
      "1000 402000\n",
      "1000 403000\n",
      "1000 404000\n",
      "1000 405000\n",
      "1000 406000\n",
      "1000 407000\n",
      "1000 408000\n",
      "1000 409000\n",
      "1000 410000\n",
      "1000 411000\n",
      "1000 412000\n",
      "1000 413000\n",
      "1000 414000\n",
      "1000 415000\n",
      "1000 416000\n",
      "1000 417000\n",
      "1000 418000\n",
      "1000 419000\n",
      "1000 420000\n",
      "1000 421000\n",
      "1000 422000\n",
      "1000 423000\n",
      "1000 424000\n",
      "1000 425000\n",
      "1000 426000\n",
      "1000 427000\n",
      "1000 428000\n",
      "1000 429000\n",
      "1000 430000\n",
      "1000 431000\n",
      "1000 432000\n",
      "1000 433000\n",
      "1000 434000\n",
      "1000 435000\n",
      "1000 436000\n",
      "1000 437000\n",
      "1000 438000\n",
      "1000 439000\n",
      "1000 440000\n",
      "1000 441000\n",
      "1000 442000\n",
      "1000 443000\n",
      "1000 444000\n",
      "1000 445000\n",
      "1000 446000\n",
      "1000 447000\n",
      "1000 448000\n",
      "1000 449000\n",
      "1000 450000\n",
      "1000 451000\n",
      "1000 452000\n",
      "1000 453000\n",
      "1000 454000\n",
      "1000 455000\n",
      "1000 456000\n",
      "1000 457000\n",
      "1000 458000\n",
      "1000 459000\n",
      "1000 460000\n",
      "1000 461000\n",
      "1000 462000\n",
      "1000 463000\n",
      "1000 464000\n",
      "1000 465000\n",
      "1000 466000\n",
      "1000 467000\n",
      "1000 468000\n",
      "1000 469000\n",
      "1000 470000\n",
      "1000 471000\n",
      "1000 472000\n",
      "1000 473000\n",
      "1000 474000\n",
      "1000 475000\n",
      "1000 476000\n",
      "1000 477000\n",
      "1000 478000\n",
      "1000 479000\n",
      "1000 480000\n",
      "1000 481000\n",
      "1000 482000\n",
      "1000 483000\n",
      "1000 484000\n",
      "1000 485000\n",
      "1000 486000\n",
      "1000 487000\n",
      "1000 488000\n",
      "1000 489000\n",
      "1000 490000\n",
      "1000 491000\n",
      "1000 492000\n",
      "1000 493000\n",
      "1000 494000\n",
      "1000 495000\n",
      "1000 496000\n",
      "1000 497000\n",
      "1000 498000\n",
      "1000 499000\n",
      "1000 500000\n",
      "1000 501000\n",
      "1000 502000\n",
      "1000 503000\n",
      "1000 504000\n",
      "1000 505000\n",
      "1000 506000\n",
      "1000 507000\n",
      "1000 508000\n",
      "1000 509000\n",
      "1000 510000\n",
      "1000 511000\n",
      "1000 512000\n",
      "1000 513000\n",
      "1000 514000\n",
      "1000 515000\n",
      "1000 516000\n",
      "1000 517000\n",
      "1000 518000\n",
      "1000 519000\n",
      "1000 520000\n",
      "1000 521000\n",
      "1000 522000\n",
      "1000 523000\n",
      "1000 524000\n",
      "1000 525000\n",
      "1000 526000\n",
      "1000 527000\n",
      "1000 528000\n",
      "1000 529000\n",
      "1000 530000\n",
      "1000 531000\n",
      "1000 532000\n",
      "1000 533000\n",
      "1000 534000\n",
      "1000 535000\n",
      "1000 536000\n",
      "1000 537000\n",
      "1000 538000\n",
      "1000 539000\n",
      "1000 540000\n",
      "1000 541000\n",
      "1000 542000\n",
      "1000 543000\n",
      "1000 544000\n",
      "1000 545000\n",
      "1000 546000\n",
      "1000 547000\n",
      "1000 548000\n",
      "1000 549000\n",
      "1000 550000\n",
      "1000 551000\n",
      "1000 552000\n",
      "1000 553000\n",
      "1000 554000\n",
      "1000 555000\n",
      "1000 556000\n",
      "1000 557000\n",
      "1000 558000\n",
      "1000 559000\n",
      "1000 560000\n",
      "1000 561000\n",
      "1000 562000\n",
      "1000 563000\n",
      "1000 564000\n",
      "1000 565000\n",
      "1000 566000\n",
      "1000 567000\n",
      "1000 568000\n",
      "1000 569000\n",
      "1000 570000\n",
      "1000 571000\n",
      "1000 572000\n",
      "1000 573000\n",
      "1000 574000\n",
      "1000 575000\n",
      "1000 576000\n",
      "1000 577000\n",
      "1000 578000\n",
      "1000 579000\n",
      "1000 580000\n",
      "1000 581000\n",
      "1000 582000\n",
      "1000 583000\n",
      "1000 584000\n",
      "1000 585000\n",
      "1000 586000\n",
      "1000 587000\n",
      "1000 588000\n",
      "1000 589000\n",
      "1000 590000\n",
      "1000 591000\n",
      "1000 592000\n",
      "1000 593000\n",
      "1000 594000\n",
      "1000 595000\n",
      "1000 596000\n",
      "1000 597000\n",
      "1000 598000\n",
      "1000 599000\n",
      "1000 600000\n",
      "1000 601000\n",
      "1000 602000\n",
      "1000 603000\n",
      "1000 604000\n",
      "1000 605000\n",
      "1000 606000\n",
      "1000 607000\n",
      "1000 608000\n",
      "1000 609000\n",
      "1000 610000\n",
      "1000 611000\n",
      "1000 612000\n",
      "1000 613000\n",
      "1000 614000\n",
      "1000 615000\n",
      "1000 616000\n",
      "1000 617000\n",
      "1000 618000\n",
      "1000 619000\n",
      "1000 620000\n",
      "1000 621000\n",
      "1000 622000\n",
      "1000 623000\n",
      "1000 624000\n",
      "1000 625000\n",
      "1000 626000\n",
      "1000 627000\n",
      "1000 628000\n",
      "1000 629000\n",
      "1000 630000\n",
      "1000 631000\n",
      "1000 632000\n",
      "1000 633000\n",
      "1000 634000\n",
      "1000 635000\n",
      "1000 636000\n",
      "1000 637000\n",
      "1000 638000\n",
      "1000 639000\n",
      "1000 640000\n",
      "1000 641000\n",
      "1000 642000\n",
      "1000 643000\n",
      "1000 644000\n",
      "1000 645000\n",
      "1000 646000\n",
      "1000 647000\n",
      "1000 648000\n",
      "1000 649000\n",
      "1000 650000\n",
      "1000 651000\n",
      "1000 652000\n",
      "1000 653000\n",
      "1000 654000\n",
      "1000 655000\n",
      "1000 656000\n",
      "1000 657000\n",
      "1000 658000\n",
      "1000 659000\n",
      "1000 660000\n",
      "1000 661000\n",
      "1000 662000\n",
      "1000 663000\n",
      "1000 664000\n",
      "1000 665000\n",
      "1000 666000\n",
      "1000 667000\n",
      "1000 668000\n",
      "1000 669000\n",
      "1000 670000\n",
      "1000 671000\n",
      "1000 672000\n",
      "1000 673000\n",
      "1000 674000\n",
      "1000 675000\n",
      "1000 676000\n",
      "1000 677000\n",
      "1000 678000\n",
      "1000 679000\n",
      "1000 680000\n",
      "1000 681000\n",
      "1000 682000\n",
      "1000 683000\n",
      "1000 684000\n",
      "1000 685000\n",
      "1000 686000\n",
      "1000 687000\n",
      "1000 688000\n",
      "1000 689000\n",
      "1000 690000\n",
      "1000 691000\n",
      "1000 692000\n",
      "1000 693000\n",
      "1000 694000\n",
      "1000 695000\n",
      "1000 696000\n",
      "1000 697000\n",
      "1000 698000\n",
      "1000 699000\n",
      "1000 700000\n",
      "1000 701000\n",
      "1000 702000\n",
      "1000 703000\n",
      "1000 704000\n",
      "1000 705000\n",
      "1000 706000\n",
      "1000 707000\n",
      "1000 708000\n",
      "1000 709000\n",
      "1000 710000\n",
      "1000 711000\n",
      "1000 712000\n",
      "1000 713000\n",
      "1000 714000\n",
      "1000 715000\n",
      "1000 716000\n",
      "1000 717000\n",
      "1000 718000\n",
      "1000 719000\n",
      "1000 720000\n",
      "1000 721000\n",
      "1000 722000\n",
      "1000 723000\n",
      "1000 724000\n",
      "1000 725000\n",
      "1000 726000\n",
      "1000 727000\n",
      "1000 728000\n",
      "1000 729000\n",
      "1000 730000\n",
      "1000 731000\n",
      "1000 732000\n",
      "1000 733000\n",
      "1000 734000\n",
      "1000 735000\n",
      "1000 736000\n",
      "1000 737000\n",
      "1000 738000\n",
      "1000 739000\n",
      "1000 740000\n",
      "1000 741000\n",
      "1000 742000\n",
      "1000 743000\n",
      "1000 744000\n",
      "1000 745000\n",
      "1000 746000\n",
      "1000 747000\n",
      "1000 748000\n",
      "1000 749000\n",
      "1000 750000\n",
      "1000 751000\n",
      "1000 752000\n",
      "1000 753000\n",
      "1000 754000\n",
      "1000 755000\n",
      "1000 756000\n",
      "1000 757000\n",
      "1000 758000\n",
      "1000 759000\n",
      "1000 760000\n",
      "1000 761000\n",
      "1000 762000\n",
      "1000 763000\n",
      "1000 764000\n",
      "1000 765000\n",
      "1000 766000\n",
      "1000 767000\n",
      "1000 768000\n",
      "1000 769000\n",
      "1000 770000\n",
      "1000 771000\n",
      "1000 772000\n",
      "1000 773000\n",
      "1000 774000\n",
      "1000 775000\n",
      "1000 776000\n",
      "1000 777000\n",
      "1000 778000\n",
      "1000 779000\n",
      "1000 780000\n",
      "1000 781000\n",
      "1000 782000\n",
      "1000 783000\n",
      "1000 784000\n",
      "1000 785000\n",
      "1000 786000\n",
      "1000 787000\n",
      "1000 788000\n",
      "1000 789000\n",
      "1000 790000\n",
      "1000 791000\n",
      "1000 792000\n",
      "1000 793000\n",
      "1000 794000\n",
      "1000 795000\n",
      "1000 796000\n",
      "1000 797000\n",
      "1000 798000\n",
      "1000 799000\n",
      "1000 800000\n",
      "1000 801000\n",
      "1000 802000\n",
      "1000 803000\n",
      "1000 804000\n",
      "1000 805000\n",
      "1000 806000\n",
      "1000 807000\n",
      "1000 808000\n",
      "1000 809000\n",
      "1000 810000\n",
      "1000 811000\n",
      "1000 812000\n",
      "1000 813000\n",
      "1000 814000\n",
      "1000 815000\n",
      "1000 816000\n",
      "1000 817000\n",
      "1000 818000\n",
      "1000 819000\n",
      "1000 820000\n",
      "1000 821000\n",
      "1000 822000\n",
      "1000 823000\n",
      "1000 824000\n",
      "1000 825000\n",
      "1000 826000\n",
      "1000 827000\n",
      "1000 828000\n",
      "1000 829000\n",
      "1000 830000\n",
      "1000 831000\n",
      "1000 832000\n",
      "1000 833000\n",
      "1000 834000\n",
      "1000 835000\n",
      "1000 836000\n",
      "1000 837000\n",
      "1000 838000\n",
      "1000 839000\n",
      "1000 840000\n",
      "1000 841000\n",
      "1000 842000\n",
      "1000 843000\n",
      "1000 844000\n",
      "1000 845000\n",
      "1000 846000\n",
      "1000 847000\n",
      "1000 848000\n",
      "1000 849000\n",
      "1000 850000\n",
      "1000 851000\n",
      "1000 852000\n",
      "1000 853000\n",
      "1000 854000\n",
      "1000 855000\n",
      "1000 856000\n",
      "1000 857000\n",
      "1000 858000\n",
      "1000 859000\n",
      "1000 860000\n",
      "1000 861000\n",
      "1000 862000\n",
      "1000 863000\n",
      "1000 864000\n",
      "1000 865000\n",
      "1000 866000\n",
      "1000 867000\n",
      "1000 868000\n",
      "1000 869000\n",
      "1000 870000\n",
      "1000 871000\n",
      "1000 872000\n",
      "1000 873000\n",
      "1000 874000\n",
      "1000 875000\n",
      "1000 876000\n",
      "1000 877000\n",
      "1000 878000\n",
      "1000 879000\n",
      "1000 880000\n",
      "1000 881000\n",
      "1000 882000\n",
      "1000 883000\n",
      "1000 884000\n",
      "1000 885000\n",
      "1000 886000\n",
      "1000 887000\n",
      "1000 888000\n",
      "1000 889000\n",
      "1000 890000\n",
      "1000 891000\n",
      "1000 892000\n",
      "1000 893000\n",
      "1000 894000\n",
      "1000 895000\n",
      "1000 896000\n",
      "1000 897000\n",
      "1000 898000\n",
      "1000 899000\n",
      "1000 900000\n",
      "1000 901000\n",
      "1000 902000\n",
      "1000 903000\n",
      "1000 904000\n",
      "1000 905000\n",
      "1000 906000\n",
      "1000 907000\n",
      "1000 908000\n",
      "1000 909000\n",
      "1000 910000\n",
      "1000 911000\n",
      "1000 912000\n",
      "1000 913000\n",
      "1000 914000\n",
      "1000 915000\n",
      "1000 916000\n",
      "1000 917000\n",
      "1000 918000\n",
      "1000 919000\n",
      "1000 920000\n",
      "1000 921000\n",
      "1000 922000\n",
      "1000 923000\n",
      "1000 924000\n",
      "1000 925000\n",
      "1000 926000\n",
      "1000 927000\n",
      "1000 928000\n",
      "1000 929000\n",
      "1000 930000\n",
      "1000 931000\n",
      "1000 932000\n",
      "1000 933000\n",
      "1000 934000\n",
      "1000 935000\n",
      "1000 936000\n",
      "1000 937000\n",
      "1000 938000\n",
      "1000 939000\n",
      "1000 940000\n",
      "1000 941000\n",
      "1000 942000\n",
      "1000 943000\n",
      "1000 944000\n",
      "1000 945000\n",
      "1000 946000\n",
      "1000 947000\n",
      "1000 948000\n",
      "1000 949000\n",
      "1000 950000\n",
      "1000 951000\n",
      "1000 952000\n",
      "1000 953000\n",
      "1000 954000\n",
      "1000 955000\n",
      "1000 956000\n",
      "1000 957000\n",
      "1000 958000\n",
      "1000 959000\n",
      "1000 960000\n",
      "1000 961000\n",
      "1000 962000\n",
      "1000 963000\n",
      "1000 964000\n",
      "1000 965000\n",
      "1000 966000\n",
      "1000 967000\n",
      "1000 968000\n",
      "1000 969000\n",
      "1000 970000\n",
      "1000 971000\n",
      "1000 972000\n",
      "1000 973000\n",
      "1000 974000\n",
      "1000 975000\n",
      "1000 976000\n",
      "1000 977000\n",
      "1000 978000\n",
      "1000 979000\n",
      "1000 980000\n",
      "1000 981000\n",
      "1000 982000\n",
      "1000 983000\n",
      "1000 984000\n",
      "1000 985000\n",
      "1000 986000\n",
      "1000 987000\n",
      "1000 988000\n",
      "1000 989000\n",
      "1000 990000\n",
      "1000 991000\n",
      "1000 992000\n",
      "1000 993000\n",
      "1000 994000\n",
      "1000 995000\n",
      "1000 996000\n",
      "1000 997000\n",
      "1000 998000\n",
      "1000 999000\n",
      "1000 1000000\n",
      "1000 1001000\n",
      "1000 1002000\n",
      "1000 1003000\n",
      "1000 1004000\n",
      "1000 1005000\n",
      "1000 1006000\n",
      "1000 1007000\n",
      "1000 1008000\n",
      "1000 1009000\n",
      "1000 1010000\n",
      "1000 1011000\n",
      "1000 1012000\n",
      "1000 1013000\n",
      "1000 1014000\n",
      "1000 1015000\n",
      "1000 1016000\n",
      "1000 1017000\n",
      "1000 1018000\n",
      "1000 1019000\n",
      "1000 1020000\n",
      "1000 1021000\n",
      "1000 1022000\n",
      "1000 1023000\n",
      "1000 1024000\n",
      "1000 1025000\n",
      "1000 1026000\n",
      "1000 1027000\n",
      "1000 1028000\n",
      "1000 1029000\n",
      "1000 1030000\n",
      "1000 1031000\n",
      "1000 1032000\n",
      "1000 1033000\n",
      "1000 1034000\n",
      "1000 1035000\n",
      "1000 1036000\n",
      "1000 1037000\n",
      "1000 1038000\n",
      "1000 1039000\n",
      "1000 1040000\n",
      "1000 1041000\n",
      "1000 1042000\n",
      "1000 1043000\n",
      "1000 1044000\n",
      "1000 1045000\n",
      "1000 1046000\n",
      "1000 1047000\n",
      "1000 1048000\n",
      "1000 1049000\n",
      "1000 1050000\n",
      "1000 1051000\n",
      "1000 1052000\n",
      "1000 1053000\n",
      "1000 1054000\n",
      "1000 1055000\n",
      "1000 1056000\n",
      "1000 1057000\n",
      "1000 1058000\n",
      "1000 1059000\n",
      "1000 1060000\n",
      "1000 1061000\n",
      "1000 1062000\n",
      "1000 1063000\n",
      "1000 1064000\n",
      "1000 1065000\n",
      "1000 1066000\n",
      "1000 1067000\n",
      "1000 1068000\n",
      "1000 1069000\n",
      "1000 1070000\n",
      "1000 1071000\n",
      "1000 1072000\n",
      "1000 1073000\n",
      "1000 1074000\n",
      "1000 1075000\n",
      "1000 1076000\n",
      "1000 1077000\n",
      "1000 1078000\n",
      "1000 1079000\n",
      "1000 1080000\n",
      "1000 1081000\n",
      "1000 1082000\n",
      "1000 1083000\n",
      "1000 1084000\n",
      "1000 1085000\n",
      "1000 1086000\n",
      "1000 1087000\n",
      "1000 1088000\n",
      "1000 1089000\n",
      "1000 1090000\n",
      "1000 1091000\n",
      "1000 1092000\n",
      "1000 1093000\n",
      "1000 1094000\n",
      "1000 1095000\n",
      "1000 1096000\n",
      "1000 1097000\n",
      "1000 1098000\n",
      "1000 1099000\n",
      "1000 1100000\n",
      "1000 1101000\n",
      "1000 1102000\n",
      "1000 1103000\n",
      "1000 1104000\n",
      "1000 1105000\n",
      "1000 1106000\n",
      "1000 1107000\n",
      "1000 1108000\n",
      "1000 1109000\n",
      "1000 1110000\n",
      "1000 1111000\n",
      "1000 1112000\n",
      "1000 1113000\n",
      "1000 1114000\n",
      "1000 1115000\n",
      "1000 1116000\n",
      "1000 1117000\n",
      "1000 1118000\n",
      "1000 1119000\n",
      "1000 1120000\n",
      "1000 1121000\n",
      "1000 1122000\n",
      "1000 1123000\n",
      "1000 1124000\n",
      "1000 1125000\n",
      "1000 1126000\n",
      "1000 1127000\n",
      "1000 1128000\n",
      "1000 1129000\n",
      "1000 1130000\n",
      "1000 1131000\n",
      "1000 1132000\n",
      "1000 1133000\n",
      "1000 1134000\n",
      "1000 1135000\n",
      "1000 1136000\n",
      "1000 1137000\n",
      "1000 1138000\n",
      "1000 1139000\n",
      "1000 1140000\n",
      "1000 1141000\n",
      "1000 1142000\n",
      "1000 1143000\n",
      "1000 1144000\n",
      "1000 1145000\n",
      "1000 1146000\n",
      "1000 1147000\n",
      "1000 1148000\n",
      "1000 1149000\n",
      "1000 1150000\n",
      "1000 1151000\n",
      "1000 1152000\n",
      "1000 1153000\n",
      "1000 1154000\n",
      "1000 1155000\n",
      "1000 1156000\n",
      "1000 1157000\n",
      "1000 1158000\n",
      "1000 1159000\n",
      "1000 1160000\n",
      "1000 1161000\n",
      "1000 1162000\n",
      "1000 1163000\n",
      "1000 1164000\n",
      "1000 1165000\n",
      "1000 1166000\n",
      "1000 1167000\n",
      "1000 1168000\n",
      "1000 1169000\n",
      "1000 1170000\n",
      "1000 1171000\n",
      "1000 1172000\n",
      "1000 1173000\n",
      "1000 1174000\n",
      "1000 1175000\n",
      "1000 1176000\n",
      "1000 1177000\n",
      "1000 1178000\n",
      "1000 1179000\n",
      "1000 1180000\n",
      "1000 1181000\n",
      "1000 1182000\n",
      "1000 1183000\n",
      "1000 1184000\n",
      "1000 1185000\n",
      "1000 1186000\n",
      "1000 1187000\n",
      "1000 1188000\n",
      "1000 1189000\n",
      "1000 1190000\n",
      "1000 1191000\n",
      "1000 1192000\n",
      "1000 1193000\n",
      "1000 1194000\n",
      "1000 1195000\n",
      "1000 1196000\n",
      "1000 1197000\n",
      "1000 1198000\n",
      "1000 1199000\n",
      "1000 1200000\n",
      "1000 1201000\n",
      "1000 1202000\n",
      "1000 1203000\n",
      "1000 1204000\n",
      "1000 1205000\n",
      "1000 1206000\n",
      "1000 1207000\n",
      "1000 1208000\n",
      "1000 1209000\n",
      "1000 1210000\n",
      "1000 1211000\n",
      "1000 1212000\n",
      "1000 1213000\n",
      "1000 1214000\n",
      "1000 1215000\n",
      "1000 1216000\n",
      "1000 1217000\n",
      "1000 1218000\n",
      "1000 1219000\n",
      "1000 1220000\n",
      "1000 1221000\n",
      "1000 1222000\n",
      "1000 1223000\n",
      "1000 1224000\n",
      "1000 1225000\n",
      "1000 1226000\n",
      "1000 1227000\n",
      "1000 1228000\n",
      "1000 1229000\n",
      "1000 1230000\n",
      "1000 1231000\n",
      "1000 1232000\n",
      "1000 1233000\n",
      "1000 1234000\n",
      "1000 1235000\n",
      "1000 1236000\n",
      "1000 1237000\n",
      "1000 1238000\n",
      "1000 1239000\n",
      "1000 1240000\n",
      "1000 1241000\n",
      "1000 1242000\n",
      "1000 1243000\n",
      "1000 1244000\n",
      "1000 1245000\n",
      "1000 1246000\n",
      "1000 1247000\n",
      "1000 1248000\n",
      "1000 1249000\n",
      "1000 1250000\n",
      "1000 1251000\n",
      "1000 1252000\n",
      "1000 1253000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenize(test_data[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m],test_en_tokenize_file_name,test_de_tokenize_file_name)\n\u001b[1;32m      2\u001b[0m tokenize(val_data[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m],val_en_tokenize_file_name,val_de_tokenize_file_name)\n\u001b[0;32m----> 3\u001b[0m tokenize(train_data[\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m],train_en_tokenize_file_name,train_de_tokenize_file_name)\n",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(data, en_file_name, de_file_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# Tokenize and save in batches\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(english_sentences), batch_size):\n\u001b[0;32m---> 11\u001b[0m     batch_english \u001b[39m=\u001b[39m batch_tokenize_input(english_sentences[i:i\u001b[39m+\u001b[39;49mbatch_size], nlp_en)\n\u001b[1;32m     12\u001b[0m     batch_german \u001b[39m=\u001b[39m batch_tokenize_input(german_sentences[i:i\u001b[39m+\u001b[39mbatch_size], nlp_de)\n\u001b[1;32m     13\u001b[0m     \u001b[39mprint\u001b[39m(batch_size,i)\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mbatch_tokenize_input\u001b[0;34m(input_texts, nlp_eng)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_tokenize_input\u001b[39m(input_texts, nlp_eng):\n\u001b[1;32m      2\u001b[0m     \u001b[39m# Tokenize the input texts in batches\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     input_tokenized \u001b[39m=\u001b[39m [nlp_eng(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m input_texts]\n\u001b[1;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m [[token\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc] \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m input_tokenized]\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_tokenize_input\u001b[39m(input_texts, nlp_eng):\n\u001b[1;32m      2\u001b[0m     \u001b[39m# Tokenize the input texts in batches\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     input_tokenized \u001b[39m=\u001b[39m [nlp_eng(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m input_texts]\n\u001b[1;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m [[token\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc] \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m input_tokenized]\n",
      "File \u001b[0;32m~/.conda/envs/environment/lib/python3.10/site-packages/spacy/language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[1;32m   1048\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1049\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg\u001b[39m.\u001b[39;49mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1051\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenize(test_data['data'],test_en_tokenize_file_name,test_de_tokenize_file_name)\n",
    "tokenize(val_data['data'],val_en_tokenize_file_name,val_de_tokenize_file_name)\n",
    "tokenize(train_data['data'],train_en_tokenize_file_name,train_de_tokenize_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_vocab(tokenized_sentences, min_freq=1):\n",
    "    # Count the frequency of each word in the dataset\n",
    "    word_freq = Counter(word for sentence in tokenized_sentences for word in sentence)\n",
    "\n",
    "    # Filter words by minimum frequency\n",
    "    vocab = [word for word, freq in word_freq.items() if freq >= min_freq]\n",
    "\n",
    "    # Create word to index mapping\n",
    "    word_to_index = {word: idx + 1 for idx, word in enumerate(vocab)}  # +1 for zero padding\n",
    "\n",
    "    return word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9507, 12945)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open('english_tokenized.pkl', 'rb') as f:\n",
    "        english_tokenized = pickle.load(f)\n",
    "\n",
    "    # Load German tokenized sentences from the pickle file\n",
    "with open('german_tokenized.pkl', 'rb') as f:\n",
    "        german_tokenized = pickle.load(f)\n",
    "\n",
    "with open('val_english_tokenized.pkl', 'rb') as f:\n",
    "        val_english_tokenized = pickle.load(f)\n",
    "\n",
    "    # Load German tokenized sentences from the pickle file\n",
    "with open('val_german_tokenized.pkl', 'rb') as f:\n",
    "        val_german_tokenized = pickle.load(f)\n",
    "\n",
    "with open('test_english_tokenized.pkl', 'rb') as f:\n",
    "        train_english_tokenized = pickle.load(f)\n",
    "\n",
    "    # Load German tokenized sentences from the pickle file\n",
    "with open('test_german_tokenized.pkl', 'rb') as f:\n",
    "        train_german_tokenized = pickle.load(f)\n",
    "\n",
    "all_english = english_tokenized + val_english_tokenized + train_english_tokenized\n",
    "all_german = german_tokenized + val_german_tokenized + train_german_tokenized\n",
    "# Create vocabulary\n",
    "eng_vocab = torchtext.vocab.build_vocab_from_iterator(all_english)\n",
    "ger_vocab = torchtext.vocab.build_vocab_from_iterator(all_german)\n",
    "# Save vocabularies\n",
    "with open('english_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(eng_vocab, f)\n",
    "\n",
    "with open('german_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(ger_vocab, f)\n",
    "len(eng_vocab),len(ger_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('english_vocab.pkl', 'rb') as f:\n",
    "    eng_vocab = pickle.load(f)\n",
    "\n",
    "# Load German tokenized sentences from the pickle file\n",
    "with open('german_vocab.pkl', 'rb') as f:\n",
    "    ger_vocab = pickle.load(f)\n",
    "\n",
    "if '<unk>' not in eng_vocab:\n",
    "    eng_vocab.insert_token('<unk>', 0)  # Adjust the index if needed\n",
    "\n",
    "if '<unk>' not in ger_vocab:\n",
    "    ger_vocab.insert_token('<unk>', 0)  # Adjust the index if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess( english_tokenized_file_name,german_tokenized_file_name,eng_vocab,ger_vocab):\n",
    "\n",
    "    with open(english_tokenized_file_name, 'rb') as f:\n",
    "        english_tokenized = pickle.load(f)\n",
    "\n",
    "    # Load German tokenized sentences from the pickle file\n",
    "    with open(german_tokenized_file_name, 'rb') as f:\n",
    "        german_tokenized = pickle.load(f)\n",
    "    # Convert words to indices\n",
    "    english_indices = [torch.tensor([eng_vocab[word] if word in eng_vocab else eng_vocab['<unk>'] for word in sentence], dtype=torch.int) for sentence in english_tokenized]\n",
    "    german_indices = [torch.tensor([ger_vocab[word] if word in ger_vocab else eng_vocab['<unk>'] for word in sentence], dtype=torch.int) for sentence in german_tokenized]\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    max_len = max(max(len(seq) for seq in english_indices), max(len(seq) for seq in german_indices))\n",
    "    english_padded = pad_sequence([torch.cat([seq, torch.zeros(max_len - len(seq))], dim=0) for seq in english_indices], batch_first=True)\n",
    "    german_padded = pad_sequence([torch.cat([seq, torch.zeros(max_len - len(seq))], dim=0) for seq in german_indices], batch_first=True)\n",
    "    return english_padded,german_padded, max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_english_padded,train_german_padded,max_len = preProcess(train_en_tokenize_file_name,train_de_tokenize_file_name,eng_vocab,ger_vocab)\n",
    "#first time\n",
    "torch.save(train_english_padded, 'english_padded.pt')\n",
    "torch.save(train_german_padded, 'german_padded.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_english_padded,val_german_padded,max_len = preProcess(val_en_tokenize_file_name,train_de_tokenize_file_name,eng_vocab,ger_vocab)\n",
    "#first time\n",
    "torch.save(train_english_padded, 'english_padded_val.pt')\n",
    "torch.save(train_german_padded, 'german_padded_val.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move data and model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_english_padded = train_english_padded.to(device)\n",
    "train_german_padded = train_german_padded.to(device)\n",
    "val_english_padded = val_english_padded.to(device)\n",
    "val_german_padded = val_german_padded.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, english_data, german_data):\n",
    "        self.english_data = english_data\n",
    "        self.german_data = german_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.english_data[idx], self.german_data[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TranslationDataset(train_english_padded, train_german_padded)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Create DataLoader\n",
    "val_dataset = TranslationDataset(val_english_padded, val_german_padded)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        return hidden, cell\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size=output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape: (N) but we want (1, N)\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # outputs shape: (1, N, hidden_size)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "        # predictions shape: (1, N, length_of_vocab)\n",
    "\n",
    "        predictions = predictions.squeeze(0)\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio = 0.5):\n",
    "        # source shape: (src_len, N)\n",
    "        # target shape: (trg_len, N)\n",
    "\n",
    "        trg_len, N = target.shape\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, N, trg_vocab_size).to(device)\n",
    "\n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        x = target[0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "\n",
    "            outputs[t] = output\n",
    "\n",
    "            # decide if we will use teacher forcing or not\n",
    "            best_guess = output.argmax(1)\n",
    "            x = target[t] if random.random() < teacher_forcing_ratio else best_guess\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "INPUT_DIM = len(eng_vocab)  # Assuming eng_vocab is your English vocabulary\n",
    "OUTPUT_DIM = len(ger_vocab)  # Assuming ger_vocab is your German vocabulary\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enc = EncoderLSTM(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
    "dec = DecoderLSTM(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, OUTPUT_DIM, N_LAYERS, DEC_DROPOUT).to(device)\n",
    "\n",
    "model = Seq2Seq(enc, dec).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1) \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        # trg = [trg len, batch size]\n",
    "        # output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (src, trg) in enumerate(iterator):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "            output = model(src, trg, 0)  # turn off teacher forcing\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m----> 6\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_dataloader, optimizer, criterion, CLIP)\n\u001b[1;32m      7\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(model, val_dataloader, criterion)\n\u001b[1;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m02\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Val. Loss: \u001b[39m\u001b[39m{\u001b[39;00mvalid_loss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[49], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m      8\u001b[0m src, trg \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mto(device), trg\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m output \u001b[39m=\u001b[39m model(src, trg)\n\u001b[1;32m     14\u001b[0m \u001b[39m# trg = [trg len, batch size]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# output = [trg len, batch size, output dim]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m output_dim \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/environment/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/environment/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 64\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, source, target, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     61\u001b[0m outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(trg_len, N, trg_vocab_size)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     63\u001b[0m \u001b[39m# last hidden state of the encoder is used as the initial hidden state of the decoder\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m hidden, cell \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(source)\n\u001b[1;32m     66\u001b[0m \u001b[39m# first input to the decoder is the <sos> tokens\u001b[39;00m\n\u001b[1;32m     67\u001b[0m x \u001b[39m=\u001b[39m target[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/environment/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/environment/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 13\u001b[0m, in \u001b[0;36mEncoderLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     11\u001b[0m     \u001b[39m# x shape: (seq_length, N)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(x))\n\u001b[1;32m     14\u001b[0m     \u001b[39m# embedding shape: (seq_length, N, embedding_size)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     outputs, (hidden, cell) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(embedding)\n",
      "File \u001b[0;32m~/.conda/envs/environment/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/environment/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/environment/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/.conda/envs/environment/lib/python3.10/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "num_epochs = 50\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_dataloader, criterion)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Val. Loss: {valid_loss:.3f}')\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'eng_to_ger_translation_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "def translate(input_sentence):\n",
    "    input_tokens = tokenize(input_sentence)\n",
    "    input_indices = torch.tensor([input_vocab[token] for token in input_tokens], dtype=torch.long).unsqueeze(1).to(device)\n",
    "\n",
    "    encoder_output, encoder_hidden = model.encoder(input_indices)\n",
    "\n",
    "    # Initialize decoder input with a start token\n",
    "    decoder_input = torch.tensor([output_vocab['<start>']], dtype=torch.long).to(device)\n",
    "    decoded_tokens = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        decoder_output, decoder_hidden = model.decoder(decoder_input, encoder_hidden)\n",
    "        _, top_index = decoder_output.topk(1)\n",
    "        decoder_input = top_index.squeeze().detach()\n",
    "        decoded_tokens.append(list(output_vocab.keys())[list(output_vocab.values()).index(int(decoder_input))])\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
